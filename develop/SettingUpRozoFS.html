

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Setting up RozoFS &mdash; RozoFS User&#39;s Guide 2.0.0 documentation</title>
  

  
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="RozoFS User&#39;s Guide 2.0.0 documentation" href="index.html"/>
        <link rel="next" title="Working with RozoFS" href="WorkingWithRozoFS.html"/>
        <link rel="prev" title="Installing RozoFS" href="InstallingRozoFS.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html" class="fa fa-home"> RozoFS User's Guide</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="AboutRozoFS.html">About RozoFS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="AboutRozoFS.html#rozofs-fundamentals">RozoFS Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="AboutRozoFS.html#rozofs-data-flow">RozoFS Data Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="AboutRozoFS.html#rozofs-data-protection">RozoFS Data Protection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="InstallingRozoFS.html">Installing RozoFS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="InstallingRozoFS.html#installing-rozofs-from-binary-packages">Installing RozoFS from Binary Packages</a></li>
<li class="toctree-l2"><a class="reference internal" href="InstallingRozoFS.html#building-and-installing-from-sources">Building and Installing from Sources</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="">Setting up RozoFS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#networking-considerations">Networking Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#preparing-nodes">Preparing Nodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-files">Configuration Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="WorkingWithRozoFS.html">Working with RozoFS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="WorkingWithRozoFS.html#manual-managing-of-rozofs-services">Manual Managing of RozoFS Services</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ConfiguringRozoFSWithRozoCLI.html">Configuring RozoFS using the Rozo CLI</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ConfiguringRozoFSWithRozoCLI.html#about-rozo-cli">About rozo CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="ConfiguringRozoFSWithRozoCLI.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="ConfiguringRozoFSWithRozoCLI.html#basic-commands">Basic Commands</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Monitoring.html">Monitoring</a><ul>
<li class="toctree-l2"><a class="reference internal" href="Monitoring.html#rozodiag-tools">Rozodiag tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="Monitoring.html#rozofs-nagios-plugins">RozoFS Nagios Plugins</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">RozoFS User's Guide</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Setting up RozoFS</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/SettingUpRozoFS.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="setting-up-rozofs">
<h1>Setting up RozoFS<a class="headerlink" href="#setting-up-rozofs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="networking-considerations">
<h2>Networking Considerations<a class="headerlink" href="#networking-considerations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="vlan-port-segregation">
<h3>Vlan/port Segregation<a class="headerlink" href="#vlan-port-segregation" title="Permalink to this headline">¶</a></h3>
<p>It is recommended to separate core traffic (application) from the SAN
traffic (RozoFS/Storage) with VLANs. It is recommended to use separate
ports for application and RozoFS/Client. When RozoFS and Storage are
co-located, they can share the same ports. However, if there are enough
available ports, it is better that each entity (RozoFS, Storage) has its
own set of ports.</p>
</div>
<div class="section" id="flow-control-802-3x">
<h3>Flow Control (802.3x)<a class="headerlink" href="#flow-control-802-3x" title="Permalink to this headline">¶</a></h3>
<p>It is <strong>mandatory</strong> to enable Flow Control on the switch ports that
handle RozoFS/Storage traffic. In addition, one must also enable Flow
Control on the NICs used by RozoFS/Storage to obtain the performance
benefit. On many networks, there can be an imbalance in the network
traffic between the devices that send network traffic and the devices
that receive the traffic. This is often the case in SAN configurations
in which many hosts (initiators such as RozoFS) are communicating with
storage devices. If senders transmit data simultaneously, they may
exceed the throughput capacity of the receiver. When this occurs, the
receiver may drop packets, forcing senders to retransmit the data after
a delay. Although this will not result in any loss of data, latency will
increase because of the retransmissions, and I/O performance will
degrade.</p>
</div>
<div class="section" id="spanning-tree-protocol">
<h3>Spanning-tree Protocol<a class="headerlink" href="#spanning-tree-protocol" title="Permalink to this headline">¶</a></h3>
<p>It is recommended to disable spanning-tree protocol (STP) on the switch
ports that connect end nodes (RozoFS clients and storage array network
interfaces). If it is still decide to enable STP on those switch ports,
one need to check for a STP vendor feature, such as PortFast, which
allows immediate transition of the ports into forwarding state.</p>
</div>
<div class="section" id="storage-and-rozofs-network-configuration">
<h3>Storage and RozoFS Network Configuration<a class="headerlink" href="#storage-and-rozofs-network-configuration" title="Permalink to this headline">¶</a></h3>
<p>RozoFS Clients/Storages node connections to the SAN network switches are
always in active-active mode. In order to leverage to Ethernet ports
utilization, the balancing among the ports is under the control of the
application and not under the control of a bonding driver (there is no
need for bonding driver with RozoFS storage node). When operating in the
default mode of RozoFs (no LACP), it is recommended that each SAN port
belongs to different VLANs. Configuration with 802.3ad (LACP) trunks is
supported, however the Ethernet ports usage will not be optimal since
the selection of a port depends on a hash applied either an MAC or IP
level.</p>
<div class="section" id="mutli-link-configuration">
<h4>Mutli-link Configuration<a class="headerlink" href="#mutli-link-configuration" title="Permalink to this headline">¶</a></h4>
<p>That configuration is the recommended one for RozoFS where there is one
separate Vlan per physical port. The following diagram describes how
storage nodes are connected toward the ToR switches. It is assumed that
the RozoFS clients reside on nodes that are connected towards the
northbound of the ToR SAN switches</p>
<div class="figure align-center">
<img alt="" src="_images/multi_link_1.png" />
</div>
<div class="figure align-center">
<img alt="" src="_images/multi_link_2.png" />
</div>
</div>
<div class="section" id="lacp-configuration">
<h4>LACP Configuration<a class="headerlink" href="#lacp-configuration" title="Permalink to this headline">¶</a></h4>
<p>In that case, the ports dedicated to the SAN (RozoFS and Storage) are
grouped in one or two LACP groups, depending if we want to separate the
RozoFS and Storage traffic or not. They can be either reside on the same
or different VLANs.</p>
<div class="figure align-center">
<img alt="" src="_images/lacp.png" />
</div>
</div>
</div>
</div>
<div class="section" id="preparing-nodes">
<h2>Preparing Nodes<a class="headerlink" href="#preparing-nodes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="enable-ha-for-rozofs-metadata-servers">
<h3>Enable HA for RozoFS metadata servers<a class="headerlink" href="#enable-ha-for-rozofs-metadata-servers" title="Permalink to this headline">¶</a></h3>
<p>RozoFS must be combined with a High-Availability (HA) software to enable a
complete storage failover solution. Indeed, unlike storage data daemons
(storaged), the exportd daemon can not be running on several nodes at the same
time. So, if you want a High-available exportd server you need to replicate the
metadata to another node and coordinate exportd actions between nodes.</p>
<p>Rozo Systems made the choice to use these 3 softwares for the <code class="docutils literal"><span class="pre">rozofs-exportd</span></code>
service:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.drbd.org">DRBD</a> - a replicated storage solution mirroring the content of block devices between hosts</li>
<li><a class="reference external" href="http://corosync.github.io/corosync/">Corosync</a> - a Group Communication System</li>
<li><a class="reference external" href="http://clusterlabs.org/">Pacemaker</a> - an open source cluster resource manager (CRM)</li>
</ul>
</div></blockquote>
<p>This chapter explain how setting up a complete RozoFS metadata failover with 2
nodes.</p>
<div class="section" id="prerequisites">
<h4>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h4>
<p>The following section informs you about system requirements, and some
prerequisites for setting up a High Availability <code class="docutils literal"><span class="pre">rozofs-exportd</span></code> service.
It also includes recommendations for this setup.</p>
<p><strong>Hardware requirements</strong></p>
<p>The following list specifies hardware requirements:</p>
<ul class="simple">
<li>At <strong>least two Linux node</strong>.</li>
<li>At <strong>least two TCP/IP communication media</strong> per node.</li>
<li>A <a class="reference external" href="https://en.wikipedia.org/wiki/STONITH">STONITH</a> mechanism. A <strong>STONITH
device</strong> is a power switch which the cluster uses to reset nodes that are
thought to be dead or behaving in a strange manner. This is the only reliable
way to ensure that no data corruption is performed
by nodes that hang and only appear to be dead.</li>
<li><strong>One empty data block device by node</strong> (partition or complete hard disk,
Logical Volume LVM..) separate from OS Hard Disk.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For more information about STONITH device, see the
<a class="reference external" href="http://clusterlabs.org/doc/crm_fencing.html">Fencing and Stonith documentation</a>.</p>
</div>
<p><strong>Software requirements</strong></p>
<ul class="simple">
<li>One Linux OS, Rozo Systems has only deployed this setup on the following OS:<ul>
<li>CentOS 6.5, 6.6 and 7</li>
<li>Debian 7 and 8</li>
</ul>
</li>
<li>Enable Network Time Protocol daemon: If nodes are not synchronized, log files
and cluster reports are very hard to analyze.</li>
</ul>
<p><strong>Other recommendations</strong></p>
<p>For a useful High Availability setup, consider the following recommendations:</p>
<ul class="simple">
<li><strong>Redundant Communication Paths</strong>: it&#8217;s strongly recommended to set up cluster
communication (corosync) via two or more redundant paths. This can be done
via:<ul>
<li>Network Device Bonding.</li>
<li>A second communication channel in Corosync.</li>
</ul>
</li>
<li>We strongly recommend <strong>multiple STONITH devices per node</strong>.</li>
</ul>
<p>The following diagram describes a ideal network configuration for a exportd HA
setup:</p>
<ul class="simple">
<li>DRBD and Corosync (Primary interface: <code class="docutils literal"><span class="pre">ring</span> <span class="pre">0</span></code>) use a bonding network device</li>
<li>A second network interface (<code class="docutils literal"><span class="pre">ring</span> <span class="pre">1</span></code>) is used by corosync if the first interface failed.</li>
<li>2 STONITH devices are used:<ul>
<li>One with the Light Out Management interface (ilo, idrac, IPMI...)</li>
<li>One with a Switched PDU</li>
</ul>
</li>
</ul>
<div class="figure align-center">
<img alt="_images/exportd-HA-ideal-setup.png" src="_images/exportd-HA-ideal-setup.png" />
</div>
</div>
<div class="section" id="meta-data-replication-with-drbd-8-4">
<h4>Meta-data Replication with DRBD 8.4<a class="headerlink" href="#meta-data-replication-with-drbd-8-4" title="Permalink to this headline">¶</a></h4>
<p><strong>About DRBD</strong></p>
<p>DRBD replicates data from the primary device to the
secondary device in a way which ensures that both copies of the data remain
identical. Think of it as a networked RAID 1. It mirrors data in real-time, so
its replication occurs continuously. Applications do not need to know that in
fact their data is stored on different disks.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For more information, see the
<a class="reference external" href="http://www.drbd.org/en/doc/users-guide-84">DRBD 8.4 online documentation</a>.</p>
</div>
<p>The following example uses two servers named <code class="docutils literal"><span class="pre">node-1</span></code> and <code class="docutils literal"><span class="pre">node-2</span></code>, and the
DRBD resource named <code class="docutils literal"><span class="pre">r0</span></code>. Each node use the device <code class="docutils literal"><span class="pre">/dev/sdd</span></code> for low level
device. It sets up <code class="docutils literal"><span class="pre">node-1</span></code> as the primary node. Be sure to
modify the instructions relative to your own configuration.</p>
<p><strong>Installing DRBD</strong></p>
<p>On both servers, install DRBD packages</p>
<p>For installing DRBD with <em>apt</em> :</p>
<div class="highlight-bash"><div class="highlight"><pre>apt-get install drbd8-utils
</pre></div>
</div>
<p>For install DRBD with <em>yum</em>:</p>
<div class="highlight-bash"><div class="highlight"><pre>yum install kmod-drbd84 drbd84-utils
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For more information about DRBD installation, see the
<a class="reference external" href="http://www.drbd.org/docs/about/">DRBD documentation</a>.</p>
</div>
<p><strong>Configuring a DRBD resource</strong></p>
<p>The DRBD configuration files are stored in the directory <code class="docutils literal"><span class="pre">/etc/drbd.d/</span></code>. There
are two configuration files which are created:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">/etc/drbd.d/r0.res</span></code> corresponds to the configuration for resource <em>r0</em>;</li>
<li><code class="docutils literal"><span class="pre">/etc/drbd.d/global_common.conf</span></code> corresponds to the global configuration of
DRBD.</li>
</ul>
<p>Create files <code class="docutils literal"><span class="pre">/etc/drbd.d/global_common.conf</span></code> and <code class="docutils literal"><span class="pre">/etc/drbd.d/r0.res</span></code> on
<code class="docutils literal"><span class="pre">node-1</span></code>, changes the lines according to your parameters, and save them.</p>
<p>Examples of configuration files for DRBD 8.4:</p>
<div class="literal-block-wrapper container" id="etc-drbd-d-global-common-conf">
<div class="code-block-caption"><span class="caption-text">/etc/drbd.d/global_common.conf</span><a class="headerlink" href="#etc-drbd-d-global-common-conf" title="Permalink to this code">¶</a></div>
<div class="highlight-ini"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50</pre></div></td><td class="code"><div class="highlight"><pre> global {
   usage-count no;
 }

 common {

   handlers {
       # handlers
       pri-lost &quot;/usr/lib/drbd/notify-pri-lost.sh; \
                 /usr/lib/drbd/notify-emergency-reboot.sh; \
                 echo b &gt; /proc/sysrq-trigger ; \
                 reboot -f&quot;;
       pri-on-incon-degr &quot;/usr/lib/drbd/notify-pri-on-incon-degr.sh; \
                          /usr/lib/drbd/notify-emergency-reboot.sh; \
                          echo b &gt; /proc/sysrq-trigger; \
                          reboot -f&quot;;
       pri-lost-after-sb &quot;/usr/lib/drbd/notify-pri-lost-after-sb.sh; \
                          /usr/lib/drbd/notify-emergency-reboot.sh; \
                          echo b &gt; /proc/sysrq-trigger; \
                          reboot -f&quot;;
       fence-peer &quot;/usr/lib/drbd/crm-fence-peer.sh&quot;;
       after-resync-target &quot;/usr/lib/drbd/crm-unfence-peer.sh&quot;;
       split-brain &quot;/usr/lib/drbd/notify-split-brain.sh root&quot;;
   }

   disk {
       on-io-error detach; # If a hard drive fails
       # The handler is supposed to reach the other node over
       # alternative communication paths and call &#39;drbdadm outdate res&#39; there
       fencing resource-only; # 2 rings should be configured in corosync
       ## Tuning recommendations
       al-extents 3389;
   }

   net {
       verify-alg crc32c;
       csums-alg crc32c;
       rr-conflict call-pri-lost;
       ### Automatic split brain recovery policies
       after-sb-0pri discard-zero-changes ;
       after-sb-1pri call-pri-lost-after-sb ;
       after-sb-2pri call-pri-lost-after-sb ;
       always-asbp;
       ## Tuning recommendations
       max-buffers 8000;
       max-epoch-size 8000;
       sndbuf-size 0;
       unplug-watermark 16;
   }
 }
</pre></div>
</td></tr></table></div>
</div>
<div class="literal-block-wrapper container" id="etc-drbd-d-r0-res">
<div class="code-block-caption"><span class="caption-text">/etc/drbd.d/r0.res</span><a class="headerlink" href="#etc-drbd-d-r0-res" title="Permalink to this code">¶</a></div>
<div class="highlight-ini"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16</pre></div></td><td class="code"><div class="highlight"><pre> resource r0 {

   protocol C;
<span class="hll">   on node-1 {
</span><span class="hll">     device      /dev/drbd0; # Block device name
</span><span class="hll">     disk        /dev/sdd;   #  Lower level device
</span><span class="hll">     address     192.168.1.1:7788; # IP address:port for data transfer
</span><span class="hll">     meta-disk   internal;  # Meta-data are stored on lower level device
</span><span class="hll">   }
</span><span class="hll">   on node-2 {
</span><span class="hll">     device      /dev/drbd0;
</span><span class="hll">     disk        /dev/sdd;
</span><span class="hll">     address     192.168.1.2:7788;
</span><span class="hll">     meta-disk   internal;
</span><span class="hll">   }
</span> }
</pre></div>
</td></tr></table></div>
</div>
<p>This file configure a DRBD resource named <code class="docutils literal"><span class="pre">r0</span></code> which uses an underlying local
disk named <code class="docutils literal"><span class="pre">/dev/sdd</span></code> on both nodes <code class="docutils literal"><span class="pre">node-1</span></code> and <code class="docutils literal"><span class="pre">node-2</span></code>.
In this example, we configure the resource to use internal meta-data (means that
DRBD stores its meta data on the same physical lower-level device as the actual
production data) and it uses TCP port <code class="docutils literal"><span class="pre">7788</span></code> for its network connections, and
binds to the IP addresses <code class="docutils literal"><span class="pre">192.168.1.1</span></code> and <code class="docutils literal"><span class="pre">192.168.1.2</span></code>, respectively.
This resource is configured to use fully synchronous replication (protocol C).</p>
<p>Copy DRBD configuration files manually to the other node (<code class="docutils literal"><span class="pre">node-2</span></code>):</p>
<div class="highlight-bash"><div class="highlight"><pre>scp /etc/drbd.d/* node-2:/etc/drbd.d/
</pre></div>
</div>
<p><strong>Enabling the DRBD resource</strong></p>
<p>Each of the following steps must be completed on both nodes.</p>
<p>Initializes the DRBD meta-data:</p>
<div class="highlight-bash"><div class="highlight"><pre>drbdadm -- --ignore-sanity-checks create-md r0
</pre></div>
</div>
<p>Attach resource <code class="docutils literal"><span class="pre">r0</span></code> to the backing device, set the replication parameters and
connect the resource to its peer:</p>
<div class="highlight-bash"><div class="highlight"><pre>drbdadm up r0
</pre></div>
</div>
<p>Start the resync process and put the device into the primary role (<code class="docutils literal"><span class="pre">node-1</span></code> in
this case) by entering the following command only on <code class="docutils literal"><span class="pre">node-1</span></code>:</p>
<div class="highlight-bash"><div class="highlight"><pre>drbdadm --force primary r0
</pre></div>
</div>
<p><strong>Creating a file system</strong></p>
<p>Create desired file system on top of your DRBD device (for example <em>ext4</em>):</p>
<div class="highlight-bash"><div class="highlight"><pre>mkfs.ext4 -b <span class="m">4096</span> -i <span class="m">4096</span> -I <span class="m">128</span> /dev/drbd0
</pre></div>
</div>
<p><strong>Testing the metadata filesystem</strong></p>
<p>If the install and configuration procedures worked as expected, you are
ready to run a basic test of the DRBD functionality. Create a mount
point on <code class="docutils literal"><span class="pre">node-1</span></code>, such as <code class="docutils literal"><span class="pre">/srv/rozofs/exports</span></code>:</p>
<div class="highlight-bash"><div class="highlight"><pre>mkdir -p /srv/rozofs/exports
</pre></div>
</div>
<p>Mount the DRBD device:</p>
<div class="highlight-bash"><div class="highlight"><pre>mount /dev/drbd0 /srv/rozofs/exports
</pre></div>
</div>
<p>In the following section, we will configure the management of high availability
with Pacemaker. So it will be necessary to have the rozofs-exportd
configuration file on both servers and this file should be identical. For that
we will move this configuration file to meta-data filesystem  and create
symbolic links on each node.</p>
<p>On <code class="docutils literal"><span class="pre">node-1</span></code> (the current primary node):</p>
<div class="highlight-bash"><div class="highlight"><pre>mv /etc/rozofs/export.conf /srv/rozofs/exports/export.conf
</pre></div>
</div>
<p>On <code class="docutils literal"><span class="pre">node-1</span></code> and <code class="docutils literal"><span class="pre">node-2</span></code>:</p>
<div class="highlight-bash"><div class="highlight"><pre>ln -sf /srv/rozofs/exports/export.conf /etc/rozofs/export.conf
</pre></div>
</div>
<p>Unmount the DRBD device on <code class="docutils literal"><span class="pre">node-1</span></code>:</p>
<div class="highlight-bash"><div class="highlight"><pre>umount /srv/rozofs/exports
</pre></div>
</div>
<p>To verify that synchronization is performed:</p>
<div class="highlight-bash"><div class="highlight"><pre>cat /proc/drbd
version: 8.4.3 <span class="o">(</span>api:1/proto:86-101<span class="o">)</span>
srcversion: 1A9F77B1CA5FF92235C2213
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:64 nr:0 dw:24 dr:93981 al:1 bm:5 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
</pre></div>
</div>
<p>The two resources are now synchronized (<code class="docutils literal"><span class="pre">UpToDate</span></code>). The initial
synchronization is performed, it is necessary to stop the DRBD service
and remove the link for the initialization script not to start the
service automatically DRBD. The service will be controlled by the
Pacemaker service.</p>
<p>Disable DRBD and rozofs-exportd init script on each meta-data node
(depending on your distribution):</p>
<p>Debian Wheezy, CentOS 6 (system V):</p>
<div class="highlight-bash"><div class="highlight"><pre>/etc/init.d/drbd stop
/etc/init.d/rozofs-exportd stop
insserv -vrf drbd rozofs-exportd
</pre></div>
</div>
<p>Debian Jessie, CentOS 7 (systemd):</p>
<div class="highlight-bash"><div class="highlight"><pre>systemctl stop rozofs-exportd drbd
systemctl disable rozofs-exportd drbd
</pre></div>
</div>
<p>It&#8217;s also necessary to create the DRBD device mountpoint directory on
<code class="docutils literal"><span class="pre">node-2</span></code> :</p>
<div class="highlight-bash"><div class="highlight"><pre>mkdir -p /srv/rozofs/exports
</pre></div>
</div>
</div>
<div class="section" id="high-availability-with-pacemaker">
<h4>High Availability with Pacemaker<a class="headerlink" href="#high-availability-with-pacemaker" title="Permalink to this headline">¶</a></h4>
<p>Pacemaker is an open-source high availability resource management tool
suitable for clusters of Linux machines. This tool can detect machine
failures with a communication system based on an exchange of UDP packets
and migrate services (resource) from one server to another.</p>
<p>The configuration of Pacemaker can be done with the
<a class="reference external" href="http://crmsh.github.io/">crmsh</a> utility (Cluster Management Shell). It
allows you to manage different resources and propagates changes on each
server automatically. The creation of a resource is done with an entry
named primitive in the configuration file. This primitive uses a script
corresponding to the application to be protected.</p>
<p>In the case of the platform, Pacemaker manages the following resources:</p>
<ul class="simple">
<li>rozofs-exportd service;</li>
<li>Mounting the file system used to store meta-data;</li>
<li>DRBD resource (<code class="docutils literal"><span class="pre">r0</span></code>), roles (master or slave);</li>
<li>Server connectivity.</li>
</ul>
<p>The following diagram describes the different resources configured and
controlled via Pacemaker. In this case, 2 servers are configured and
<code class="docutils literal"><span class="pre">node-1</span></code> is the master server.</p>
<img alt="_images/pacemaker.png" class="align-center" src="_images/pacemaker.png" />
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In our case we want setting up the cluster to move all the resources when
we don&#8217;t have enough connectivity with the storaged nodes. Therefore we use
ping resource with the list of storaged nodes IP.</p>
</div>
<p><strong>Installing Corosync and Pacemaker packages</strong></p>
<p>On both servers, install the following packages:</p>
<blockquote>
<div><ul class="simple">
<li>pacemaker</li>
<li>crmsh</li>
<li>corosync</li>
<li>fence-agents, ipmitool (STONITH)</li>
<li>resource-agents</li>
<li>fping</li>
<li>rozofs-exportd</li>
</ul>
</div></blockquote>
<p><strong>Create Cluster Authorization Key</strong></p>
<p>The first component to configure is Corosync. It manages the
infrastructure of the cluster, i.e. the status of nodes and their
operation. For this, we must generate an authentication key that is
shared by all the machines in the cluster. The <code class="docutils literal"><span class="pre">corosync-keygen</span></code>
utility can be use to generate this key and then copy it to the other
nodes.</p>
<p>Create key on <code class="docutils literal"><span class="pre">node-1</span></code>:</p>
<div class="highlight-bash"><div class="highlight"><pre>corosync-keygen
</pre></div>
</div>
<p>Copy the key manually to the other node:</p>
<div class="highlight-bash"><div class="highlight"><pre>scp /etc/corosync/authkey root@node-2:/etc/corosync/authkey
</pre></div>
</div>
<p><strong>Configuring Corosync</strong></p>
<p>Besides copying the key, you also have to modify the corosync
configuration file which stored in <code class="docutils literal"><span class="pre">/etc/corosync/corosync.conf</span></code>.</p>
<p>Edit your <code class="docutils literal"><span class="pre">corosync.conf</span></code> with the following (example with <strong>unicast</strong> and
<strong>corosync version 2</strong>):</p>
<div class="literal-block-wrapper container" id="corosync-conf">
<div class="code-block-caption"><span class="caption-text">/etc/corosync/corosync.conf</span><a class="headerlink" href="#corosync-conf" title="Permalink to this code">¶</a></div>
<div class="highlight-ini"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64</pre></div></td><td class="code"><div class="highlight"><pre> totem {
     version: 2

     # How long before declaring a token lost (ms)
     token: 6000
     # How many token retransmits before forming a new configuration
     token_retransmits_before_loss_const: 10

     clear_node_high_bit: yes

     # This specifies the mode of redundant ring
     rrp_mode: passive

<span class="hll">     # The following values need to be set based on your environment
</span><span class="hll">     interface {
</span><span class="hll">         ringnumber: 0
</span><span class="hll">         bindnetaddr: 192.168.1.0
</span><span class="hll">         mcastport: 5405
</span><span class="hll">     }
</span><span class="hll">
</span><span class="hll">     interface {
</span><span class="hll">         ringnumber: 1
</span><span class="hll">         bindnetaddr: 192.168.2.0
</span><span class="hll">         mcastport: 5407
</span><span class="hll">     }
</span><span class="hll">     transport: udpu
</span> }

 quorum {
     provider: corosync_votequorum
     # Only valid with 2 nodes
     expected_votes: 2
     two_node: 1
 }

<span class="hll"> nodelist {
</span><span class="hll">     node {
</span><span class="hll">         ring0_addr: 192.168.1.1
</span><span class="hll">         ring1_addr: 192.168.2.1
</span><span class="hll">         name: node-1
</span><span class="hll">         nodeid: 1
</span><span class="hll">     }
</span><span class="hll">     node {
</span><span class="hll">         ring0_addr: 192.168.1.2
</span><span class="hll">         ring1_addr: 192.168.2.2
</span><span class="hll">         name: node-2
</span><span class="hll">         nodeid: 2
</span><span class="hll">     }
</span><span class="hll"> }
</span>
 logging {
     fileline: off
     to_stderr: no
     to_logfile: no
     #logfile: /var/log/corosync/corosync.log
     to_syslog: yes
     syslog_facility: daemon
     debug: off
     timestamp: on
     logger_subsys {
       subsys: QUORUM
       debug: off
     }
 }
</pre></div>
</td></tr></table></div>
</div>
<p>Copy the <code class="docutils literal"><span class="pre">corosync.conf</span></code> manually to the other node:</p>
<div class="highlight-bash"><div class="highlight"><pre>scp /etc/corosync/corosync.conf root@node-2:/etc/corosync/corosync.conf
</pre></div>
</div>
<p>By default, the Corosync service is disabled. On both servers, change that
by editing <code class="docutils literal"><span class="pre">/etc/default/corosync</span></code> and change the value of START to yes if
needed:</p>
<div class="literal-block-wrapper container" id="id1">
<div class="code-block-caption"><span class="caption-text">/etc/default/corosync</span><a class="headerlink" href="#id1" title="Permalink to this code">¶</a></div>
<div class="highlight-bash"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span class="nv">START</span><span class="o">=</span>yes
</pre></div>
</td></tr></table></div>
</div>
<p><strong>Starting Corosync</strong></p>
<p>Corosync is started as a regular system service. Depending on your
distribution, it may ship with a LSB init script, an upstart job, or a
systemd unit file. Either way, the service is usually named corosync.</p>
<p>Examples:</p>
<div class="highlight-bash"><div class="highlight"><pre>/etc/init.d/corosync start
service corosync start
start corosync
systemctl start corosync
</pre></div>
</div>
<p>You can now check the ring status manually with <code class="docutils literal"><span class="pre">corosync-cfgtool</span></code>:</p>
<div class="highlight-bash"><div class="highlight"><pre>corosync-cfgtool -s
Printing ring status.
Local node ID 1
RING ID 0
        <span class="nv">id</span>      <span class="o">=</span> 192.168.1.1
        <span class="nv">status</span>  <span class="o">=</span> ring <span class="m">0</span> active with no faults
RING ID 1
        <span class="nv">id</span>      <span class="o">=</span> 192.168.2.1
        <span class="nv">status</span>  <span class="o">=</span> ring <span class="m">1</span> active with no faults
</pre></div>
</div>
<p><strong>Configuring Pacemaker</strong></p>
<p>Once the Pacemaker cluster is set up and before configuring the
different resources and constraints of the Pacemaker cluster, it is
necessary to have the OCF scripts for exportd on each server. This script is
enable to start, stop and monitor the exportd daemon. This script is installed
by default with the rozofs-exportd package
(<code class="docutils literal"><span class="pre">/usr/lib/ocf/resource.d/heartbeat/exportd</span></code>).</p>
<p>To set the cluster properties, create cluster resources configuration file
<code class="docutils literal"><span class="pre">crm.conf</span></code> and changes the lines according to your parameters, and save it.</p>
<div class="highlight-bash"><div class="highlight"><pre>property stonith-enabled<span class="o">=</span><span class="s2">&quot;false&quot;</span> no-quorum-policy<span class="o">=</span><span class="s2">&quot;ignore&quot;</span>

rsc_defaults migration-threshold<span class="o">=</span><span class="m">10</span> failure-timeout<span class="o">=</span>60

primitive p-ping ocf:pacemaker:ping params  <span class="se">\</span>
<span class="nv">host_list</span><span class="o">=</span><span class="s2">&quot;192.168.1.1 192.168.1.2 192.168.1.3 192.168.1.4&quot;</span> <span class="se">\</span>
<span class="nv">multiplier</span><span class="o">=</span><span class="s2">&quot;100&quot;</span> <span class="nv">dampen</span><span class="o">=</span><span class="s2">&quot;5s&quot;</span> <span class="se">\</span>
op start <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;60&quot;</span> op monitor <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;5s&quot;</span> <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;60&quot;</span>

clone c-ping p-ping meta <span class="nv">interleave</span><span class="o">=</span><span class="s2">&quot;true&quot;</span>

primitive p-drbd-r0 ocf:linbit:drbd params <span class="nv">drbd_resource</span><span class="o">=</span><span class="s2">&quot;r0&quot;</span> <span class="se">\</span>
<span class="nv">adjust_master_score</span><span class="o">=</span><span class="s2">&quot;0 10 1000 10000&quot;</span> op start <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;240&quot;</span> <span class="se">\</span>
op stop <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;100&quot;</span> op notify <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;0&quot;</span> <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;90&quot;</span> <span class="se">\</span>
op monitor <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;10&quot;</span> <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;20&quot;</span> <span class="nv">role</span><span class="o">=</span><span class="s2">&quot;Master&quot;</span> <span class="se">\</span>
op monitor <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;20&quot;</span> <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;20&quot;</span> <span class="nv">role</span><span class="o">=</span><span class="s2">&quot;Slave&quot;</span>

ms ms-drbd-r0 p-drbd-r0 meta master-max<span class="o">=</span><span class="s2">&quot;1&quot;</span> master-node-max<span class="o">=</span><span class="s2">&quot;1&quot;</span> <span class="se">\</span>
clone-max<span class="o">=</span><span class="s2">&quot;2&quot;</span> clone-node-max<span class="o">=</span><span class="s2">&quot;1&quot;</span> <span class="nv">notify</span><span class="o">=</span><span class="s2">&quot;true&quot;</span> <span class="se">\</span>
globally-unique<span class="o">=</span><span class="s2">&quot;false&quot;</span> <span class="nv">interleave</span><span class="o">=</span><span class="s2">&quot;true&quot;</span>

primitive p-fs-exportd ocf:heartbeat:Filesystem params <span class="nv">device</span><span class="o">=</span><span class="s2">&quot;/dev/drbd0&quot;</span> <span class="se">\</span>
<span class="nv">directory</span><span class="o">=</span><span class="s2">&quot;/srv/rozofs/exports&quot;</span> <span class="nv">fstype</span><span class="o">=</span><span class="s2">&quot;ext4&quot;</span> <span class="nv">options</span><span class="o">=</span><span class="s2">&quot;user_xattr,noatime&quot;</span> <span class="se">\</span>
op start <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;60&quot;</span> op stop <span class="nv">timeout</span><span class="o">=</span><span class="s2">&quot;60&quot;</span> op monitor <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;10&quot;</span>

primitive exportd-rozofs ocf:heartbeat:exportd params  <span class="se">\</span>
<span class="nv">conffile</span><span class="o">=</span><span class="s2">&quot;/etc/rozofs/export.conf&quot;</span> op monitor <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;10s&quot;</span>

group grp-exportd p-fs-exportd exportd-rozofs

colocation c-grp-exportd-on-drbd-r0 inf: grp-exportd ms-drbd-r0:Master

order o-drbd-r0-before-grp-exportd inf: ms-drbd-r0:promote grp-exportd:start

location loc-ms-drbd-r0-needs-ping ms-drbd-r0 <span class="se">\</span>
rule -inf: not_defined pingd or pingd lt 200
</pre></div>
</div>
<p>Load this configuration with the following command:</p>
<div class="highlight-bash"><div class="highlight"><pre>crm configure load replace crm.conf
</pre></div>
</div>
<p>Once all the primitives and constraints are loaded, it is possible to
check the correct operations of the cluster with the following command:</p>
<div class="highlight-bash"><div class="highlight"><pre>crm_mon -1

Last updated: Mon Jun  <span class="m">6</span> 09:46:39 2016
Last change: Wed Jun  <span class="m">1</span> 15:14:04 <span class="m">2016</span> by root via cibadmin on node-2
Stack: corosync
Current DC: node-1 <span class="o">(</span>version 1.1.14-70404b0<span class="o">)</span> - partition with quorum
<span class="m">2</span> nodes and <span class="m">6</span> resources configured

Online: <span class="o">[</span> node-1 node-2 <span class="o">]</span>

 Resource Group: grp-exportd
     p-fs-exportd       <span class="o">(</span>ocf::heartbeat:Filesystem<span class="o">)</span>:    Started node-1
     exportd-rozofs     <span class="o">(</span>ocf::heartbeat:exportd<span class="o">)</span>:       Started node-1
 Master/Slave Set: ms-drbd-r0 <span class="o">[</span>p-drbd-r0<span class="o">]</span>
     Masters: <span class="o">[</span> node-1 <span class="o">]</span>
     Slaves: <span class="o">[</span> node-2 <span class="o">]</span>
 Clone Set: c-ping <span class="o">[</span>p-ping<span class="o">]</span>
     Started: <span class="o">[</span> node-1 node-2 <span class="o">]</span>
</pre></div>
</div>
<p><strong>Adding a STONITH resource (example with IPMI)</strong></p>
<p>Before using STONITH with IPMI, you must configure the network used by the IPMI
devices and the IPMI devices on each node.</p>
<p>After doing this you can add the STONITH resources in the Pacemaker cluster
configuration:</p>
<div class="highlight-bash"><div class="highlight"><pre>crm configure primitive fence-node-1 stonith:fence_ipmilan params <span class="se">\</span>
<span class="nv">pcmk_host_list</span><span class="o">=</span><span class="s2">&quot;node-1&quot;</span> <span class="nv">ipaddr</span><span class="o">=</span><span class="s2">&quot;192.168.100.1&quot;</span> <span class="se">\</span>
<span class="nv">login</span><span class="o">=</span><span class="s2">&quot;login&quot;</span> <span class="nv">passwd</span><span class="o">=</span><span class="s2">&quot;passwd&quot;</span> <span class="nv">lanplus</span><span class="o">=</span><span class="s2">&quot;true&quot;</span> <span class="se">\</span>
<span class="nv">pcmk_reboot_action</span><span class="o">=</span><span class="s2">&quot;off&quot;</span> op monitor <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;3600s&quot;</span>

crm configure location loc-fence-node-1 fence-node-1 -inf: node-1

crm configure primitive fence-node-2 stonith:fence_ipmilan params <span class="se">\</span>
<span class="nv">pcmk_host_list</span><span class="o">=</span><span class="s2">&quot;node-2&quot;</span> <span class="nv">ipaddr</span><span class="o">=</span><span class="s2">&quot;192.168.100.2&quot;</span> <span class="se">\</span>
<span class="nv">login</span><span class="o">=</span><span class="s2">&quot;login&quot;</span> <span class="nv">passwd</span><span class="o">=</span><span class="s2">&quot;passwd&quot;</span> <span class="nv">lanplus</span><span class="o">=</span><span class="s2">&quot;true&quot;</span> <span class="se">\</span>
<span class="nv">pcmk_reboot_action</span><span class="o">=</span><span class="s2">&quot;off&quot;</span> op monitor <span class="nv">interval</span><span class="o">=</span><span class="s2">&quot;3600s&quot;</span>

crm configure location loc-fence-node-2 fence-node-2 -inf: node-2
</pre></div>
</div>
<p>Set the global cluster option stonith-enabled to true:</p>
<div class="highlight-bash"><div class="highlight"><pre>crm configure property stonith-enabled<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p><strong>Testing your cluster configuration</strong></p>
<p>Now, you can testing your cluster configuration.</p>
<p>Migrating <cite>rozofs-exportd</cite> resource:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c"># Get current location of exportd-rozofs</span>
crm resource status exportd-rozofs
  resource exportd-rozofs is running on: node-1
<span class="c"># Migrate exportd-rozofs resource to node-2</span>
crm resource migrate exportd-rozofs node-2
<span class="c"># Get current location of exportd-rozofs</span>
crm resource status exportd-rozofs
  resource exportd-rozofs is running on: node-2
<span class="c"># Very important, remove the new constraint</span>
crm resource unmigrate exportd-rozofs
</pre></div>
</div>
<p>Simulate <cite>rozofs-exportd</cite> failure:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c"># Manually stop rozofs-exportd service</span>
<span class="c"># on the node who the resource is currently running!</span>
<span class="c"># (without using crm utility)</span>
/etc/init.d/rozofs-exportd stop

<span class="c"># Check that the resource agent has restarted `rozofs-exportd` service.</span>
crm_mon -rfn1

Node node-2: online
        exportd-rozofs  <span class="o">(</span>ocf::heartbeat:exportd<span class="o">)</span> Started
        p-ping:1        <span class="o">(</span>ocf::pacemaker:ping<span class="o">)</span> Started
        p-fs-exportd    <span class="o">(</span>ocf::heartbeat:Filesystem<span class="o">)</span> Started
        p-drbd-r0:1     <span class="o">(</span>ocf::linbit:drbd<span class="o">)</span> Master
Node node-1: online
        p-ping:0        <span class="o">(</span>ocf::pacemaker:ping<span class="o">)</span> Started
        p-drbd-r0:0     <span class="o">(</span>ocf::linbit:drbd<span class="o">)</span> Slave

Inactive resources:

Migration summary:
* Node node-2:
   exportd-rozofs: migration-threshold<span class="o">=</span><span class="m">10</span> fail-count<span class="o">=</span><span class="m">1</span> last-failure<span class="o">=</span><span class="s1">&#39;Tue Jun 28 08:55:46 2016&#39;</span>
* Node node-1:

Failed actions:
    exportd-rozofs_monitor_10000 <span class="o">(</span><span class="nv">node</span><span class="o">=</span>node-2, <span class="nv">call</span><span class="o">=</span>22, <span class="nv">rc</span><span class="o">=</span>7, <span class="nv">status</span><span class="o">=</span><span class="nb">complete</span><span class="o">)</span>: not running
</pre></div>
</div>
<p>Testing stonith:</p>
<div class="highlight-bash"><div class="highlight"><pre>crm node fence &lt;nodename&gt;
killall -9 corosync
</pre></div>
</div>
</div>
</div>
<div class="section" id="storaged-nodes">
<h3>Storaged Nodes<a class="headerlink" href="#storaged-nodes" title="Permalink to this headline">¶</a></h3>
<p>Storaged Storaged nodes should have appropriate free space on disks. The
storaged service stores transformed data as files on a common file
system (ext4). It is important to dedicate file systems used by storaged
service exclusively to it (use a Logical Volume or dedicated partition).
It is necessary to manage the free space properly.</p>
</div>
</div>
<div class="section" id="configuration-files">
<h2>Configuration Files<a class="headerlink" href="#configuration-files" title="Permalink to this headline">¶</a></h2>
<div class="section" id="exportd-configuration-file">
<h3>Exportd Configuration File<a class="headerlink" href="#exportd-configuration-file" title="Permalink to this headline">¶</a></h3>
<p>The configuration file of exportd (<code class="docutils literal"><span class="pre">export.conf</span></code>) consists of 3 types
of information :</p>
<ul class="simple">
<li>the redundancy configuration chosen (layout)</li>
<li>the list of storage volumes used to store data (volumes)</li>
<li>list of file systems exported (exports)</li>
</ul>
<p>Redundancy Configuration (layout): the <strong>layout</strong> allows you to specify
the configuration of redundancy RozoFS. There are 3 redundancy
configurations that are possible :</p>
<ul class="simple">
<li>layout=0; cluster(s) of 4 storage locations, 3 are used for each
write and 2 for each read</li>
<li>layout=1; cluster(s) of 8 storage locations, 6 are used for each
write and 4 for each read</li>
<li>layout=2; cluster(s) 16 storage locations, 12 are used for each write
and 8 for each read</li>
</ul>
<p>List of storage volumes (volumes): The list of all the storage
<strong>volumes</strong> used by exportd is grouped under the volumes list. A volume
in the list is identified by a unique identification number (VID) and
contains one or more <strong>clusters</strong> identified by a unique identification
number (CID) consisting of 4, 8 or 16 storage locations according to the
layout you have chosen. Each storage location in a cluster is defined
with the SID (the storage unique identifier within the cluster) and its
network name (or IP address).</p>
<p>List of exported file systems (exports): The exportd daemon can export
one or more file systems. Each exported file system is defined by the
absolute path to the local directory that contains specific metadata for
this file system.</p>
<p>Here is the an example of configuration file (<code class="docutils literal"><span class="pre">export.conf</span></code>) for
exportd daemon:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c"># rozofs export daemon configuration file</span>

<span class="nv">layout</span> <span class="o">=</span> <span class="m">0</span> <span class="p">;</span> <span class="c"># (inverse = 2, forward = 3, safe = 4)</span>

<span class="nv">volumes</span> <span class="o">=</span> <span class="c"># List of volumes</span>
<span class="o">(</span>
    <span class="o">{</span>
        <span class="c"># First volume</span>
        <span class="nv">vid</span> <span class="o">=</span> <span class="m">1</span> <span class="p">;</span> <span class="c"># Volume identifier = 1</span>
        <span class="nv">cids</span><span class="o">=</span>     <span class="c"># List of clusters for the volume 1</span>
        <span class="o">(</span>
            <span class="o">{</span>
                <span class="c"># First cluster of volume 1</span>
                <span class="nv">cid</span> <span class="o">=</span> 1<span class="p">;</span>  <span class="c"># Cluster identifier = 1</span>
                <span class="nv">sids</span> <span class="o">=</span>    <span class="c"># List of storages for the cluster 1</span>
                <span class="o">(</span>
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 01<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-1-1&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 02<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-1-2&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 03<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-1-3&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 04<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-1-4&quot;</span><span class="p">;</span><span class="o">}</span>
                <span class="o">)</span><span class="p">;</span>
            <span class="o">}</span>,
            <span class="o">{</span>
                <span class="c"># Second cluster of volume 1</span>
                <span class="nv">cid</span> <span class="o">=</span> 2<span class="p">;</span> <span class="c"># Cluster identifier = 2</span>
                <span class="nv">sids</span> <span class="o">=</span>   <span class="c"># List of storages for the cluster 2</span>
                <span class="o">(</span>
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 01<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-2-1&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 02<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-2-2&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 03<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-2-3&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 04<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-2-4&quot;</span><span class="p">;</span><span class="o">}</span>
                <span class="o">)</span><span class="p">;</span>
            <span class="o">}</span>
        <span class="o">)</span><span class="p">;</span>
    <span class="o">}</span>,
    <span class="o">{</span>
        <span class="c"># Second volume</span>
        <span class="nv">vid</span> <span class="o">=</span> 2<span class="p">;</span> <span class="c"># Volume identifier = 2</span>
        <span class="nv">cids</span> <span class="o">=</span>   <span class="c"># List of clusters for the volume 2</span>
        <span class="o">(</span>
            <span class="o">{</span>
                <span class="c"># First cluster of volume 2</span>
                <span class="nv">cid</span> <span class="o">=</span> 3<span class="p">;</span> <span class="c"># Cluster identifier = 3</span>
                <span class="nv">sids</span> <span class="o">=</span>   <span class="c"># List of storages for the cluster 3</span>
                <span class="o">(</span>
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 01<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-3-1&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 02<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-3-2&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 03<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-3-3&quot;</span><span class="p">;</span><span class="o">}</span>,
                    <span class="o">{</span><span class="nv">sid</span> <span class="o">=</span> 04<span class="p">;</span> <span class="nv">host</span> <span class="o">=</span> <span class="s2">&quot;storage-node-3-4&quot;</span><span class="p">;</span><span class="o">}</span>
                <span class="o">)</span><span class="p">;</span>
            <span class="o">}</span>
        <span class="o">)</span><span class="p">;</span>
    <span class="o">}</span>
<span class="o">)</span><span class="p">;</span>

<span class="c"># List of exported filesystem</span>
<span class="nv">exports</span> <span class="o">=</span> <span class="o">(</span>

  <span class="c"># First filesystem exported</span>
  <span class="o">{</span><span class="nv">eid</span> <span class="o">=</span> 1<span class="p">;</span> <span class="nv">root</span> <span class="o">=</span> <span class="s2">&quot;/srv/rozofs/exports/export_1&quot;</span><span class="p">;</span> <span class="nv">md5</span><span class="o">=</span><span class="s2">&quot;AyBvjVmNoKAkLQwNa2c&quot;</span><span class="p">;</span>
   <span class="nv">squota</span><span class="o">=</span><span class="s2">&quot;128G&quot;</span><span class="p">;</span> <span class="nv">hquota</span><span class="o">=</span><span class="s2">&quot;256G&quot;</span><span class="p">;</span> <span class="nv">vid</span><span class="o">=</span>1<span class="p">;</span><span class="o">}</span>,
  <span class="c"># Second filesystem exported</span>
  <span class="o">{</span><span class="nv">eid</span> <span class="o">=</span> 2<span class="p">;</span> <span class="nv">root</span> <span class="o">=</span> <span class="s2">&quot;/srv/rozofs/exports/export_2&quot;</span><span class="p">;</span> <span class="nv">md5</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">;</span>
  <span class="nv">squota</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">;</span> <span class="nv">hquota</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">;</span> <span class="nv">vid</span><span class="o">=</span>2<span class="p">;</span><span class="o">}</span>
<span class="o">)</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="storaged-configuration-file">
<h3>Storaged Configuration File<a class="headerlink" href="#storaged-configuration-file" title="Permalink to this headline">¶</a></h3>
<p>The configuration file of the <strong>storaged</strong> daemon (<code class="docutils literal"><span class="pre">storage.conf</span></code>)
must be completed on each physical server storage where storaged daemon
is used. It contains two informations:</p>
<ul class="simple">
<li>ports; list of TCP ports used to receive requests to write and read
from clients using rozofsmount</li>
<li>storages; list of local storage locations used to store the
transformed data (projections)</li>
</ul>
<p>List of local storage locations (storages): All of storage locations
used by the storaged daemon on a physical server are grouped under the
storages list. The storages list consists of one or more storage
locations. Each storage location is defined by the CID (unique
identification number of the cluster to which it belongs) and SID (the
storage unique identifier within the cluster) and the absolute path to
the local directory that will contain the specific encoded data for this
storage.</p>
<p>Configuration file example (<code class="docutils literal"><span class="pre">storage.conf</span></code>) for one storaged daemon:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c"># rozofs storage daemon configuration file.</span>

<span class="c"># listen: (mandatory)</span>
<span class="c">#   Specifies list of IP(s) (or hostname(s)) and port(s) the storio</span>
<span class="c">#   process should listen on for receive write and read requests from</span>
<span class="c">#   clients.</span>

<span class="nv">listen</span> <span class="o">=</span> <span class="o">(</span>
   <span class="o">{</span>
      <span class="nv">addr</span> <span class="o">=</span> <span class="s2">&quot;*&quot;</span><span class="p">;</span>
      <span class="nv">port</span> <span class="o">=</span> 41001<span class="p">;</span>
   <span class="o">}</span>
<span class="o">)</span><span class="p">;</span>

<span class="c"># storages:</span>
<span class="c">#   It&#39;s the list of local storage managed by this storaged.</span>

<span class="nv">storages</span> <span class="o">=</span> <span class="o">(</span>
  <span class="o">{</span><span class="nv">cid</span> <span class="o">=</span> 1<span class="p">;</span> <span class="nv">sid</span> <span class="o">=</span> 1<span class="p">;</span> <span class="nv">root</span> <span class="o">=</span> <span class="s2">&quot;/srv/rozofs/storages/storage_1-1&quot;</span><span class="p">;</span><span class="o">}</span>,
  <span class="o">{</span><span class="nv">cid</span> <span class="o">=</span> 2<span class="p">;</span> <span class="nv">sid</span> <span class="o">=</span> 1<span class="p">;</span> <span class="nv">root</span> <span class="o">=</span> <span class="s2">&quot;/srv/rozofs/storages/storage_2-1&quot;</span><span class="p">;</span><span class="o">}</span>
<span class="o">)</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="WorkingWithRozoFS.html" class="btn btn-neutral float-right" title="Working with RozoFS">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="InstallingRozoFS.html" class="btn btn-neutral" title="Installing RozoFS"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Rozo Systems S.A.S.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'2.0.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>